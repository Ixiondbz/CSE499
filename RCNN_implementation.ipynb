{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN implementation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/12XSoYWywzuQPcbEeGu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ixiondbz/CSE499/blob/main/RCNN_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "nBK6o0bPmh6L",
        "outputId": "fa662bde-a39a-487d-96c7-7405fdd5798c"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install -U plotly\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import pickle\n",
        "import math\n",
        "import cv2\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
        "from keras.utils.layer_utils import get_source_inputs\n",
        "#from keras.engine.topology import get_source_inputs\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.utils import generic_utils\n",
        "#tf.keras.layers.InputSpec\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras import initializers, regularizers"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.0.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-16b62136f430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SGD' from 'keras.optimizers' (/usr/local/lib/python3.7/dist-packages/keras/optimizers.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDNPG5s0mnM5",
        "outputId": "de309abb-489d-4c3c-c449-26a33a96dc33"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "folder = \"MyDrive/MIDOG_Challenge\" #@param {type:\"string\"}\n",
        "midog_folder = Path(\"/drive\") / Path(folder)\n",
        "\n",
        "print(list(midog_folder.glob(\"*.*\")))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "[PosixPath('/drive/MyDrive/MIDOG_Challenge/MIDOG.sqlite'), PosixPath('/drive/MyDrive/MIDOG_Challenge/MIDOG.json')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gytRH8aXmozQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3bf5e6-cae8-43e4-86ae-d665bcdd012d"
      },
      "source": [
        "!apt-get install python3-openslide\n",
        "from openslide import open_slide"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-openslide is already the newest version (1.1.1-2ubuntu4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC_Bhdaxm32K"
      },
      "source": [
        "image_folder = midog_folder / \"images\"\n",
        "\n",
        "hamamatsu_rx_ids = list(range(0, 51))\n",
        "hamamatsu_360_ids = list(range(51, 101))\n",
        "aperio_ids = list(range(101, 151))\n",
        "leica_ids = list(range(151, 201))\n",
        "\n",
        "\n",
        "annotation_file = midog_folder / \"MIDOG.json\"\n",
        "rows = []\n",
        "with open(annotation_file) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    categories = {1: 'mitotic figure', 2: 'hard negative'}\n",
        "\n",
        "    for row in data[\"images\"]:\n",
        "        file_name = row[\"file_name\"]\n",
        "        image_id = row[\"id\"]\n",
        "        width = row[\"width\"]\n",
        "        height = row[\"height\"]\n",
        "\n",
        "        scanner  = \"Hamamatsu XR\"\n",
        "        if image_id in hamamatsu_360_ids:\n",
        "            scanner  = \"Hamamatsu S360\"\n",
        "        if image_id in aperio_ids:\n",
        "            scanner  = \"Aperio CS\"\n",
        "        if image_id in leica_ids:\n",
        "            scanner  = \"Leica GT450\"\n",
        "         \n",
        "        for annotation in [anno for anno in data['annotations'] if anno[\"image_id\"] == image_id]:\n",
        "            box = annotation[\"bbox\"]\n",
        "            cat = categories[annotation[\"category_id\"]]\n",
        "\n",
        "            rows.append([file_name, image_id, width, height, box, cat, scanner])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"file_name\", \"image_id\", \"width\", \"height\", \"box\", \"cat\", \"scanner\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCFhmdrmsBHp"
      },
      "source": [
        "!pip install -U object-detection-fastai\n",
        "\n",
        "from object_detection_fastai.helper.wsi_loader import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmtC7JGGqA-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dde0829-b1b0-46ac-c72b-d6ce1522d361"
      },
      "source": [
        "def sample_function(y, classes, size, level_dimensions, level):\n",
        "    width, height = level_dimensions[level]\n",
        "    if len(y[0]) == 0:\n",
        "        return randint(0, width - size[0]), randint(0, height -size[1])\n",
        "    else:\n",
        "        #if randint(0, 5) < 2:\n",
        "        if True:\n",
        "            class_id = np.random.choice(classes, 1)[0] # select a random class\n",
        "            ids = np.array(y[1]) == class_id # filter the annotations according to the selected class\n",
        "            xmin, ymin, _, _ = np.array(y[0])[ids][randint(0, np.count_nonzero(ids) - 1)] # randomly select one of the filtered annotatons as seed for the training patch\n",
        "            \n",
        "            # To have the selected annotation not in the center of the patch and an random offset.\n",
        "            xmin += random.randint(-size[0]/2, size[0]/2) \n",
        "            ymin += random.randint(-size[1]/2, size[1]/2)\n",
        "            xmin, ymin = max(0, int(xmin - size[0] / 2)), max(0, int(ymin -size[1] / 2))\n",
        "            xmin, ymin = min(xmin, width - size[0]), min(ymin, height - size[1])\n",
        "            return xmin, ymin\n",
        "        else:\n",
        "            return randint(0, width - size[0]), randint(0, height -size[1])\n",
        "\n",
        "            \n",
        "def create_wsi_container(annotations_df: pd.DataFrame):\n",
        "    container = []\n",
        "\n",
        "    for image_name in tqdm(annotations_df[\"file_name\"].unique()):\n",
        "        image_annos = annotations_df[annotations_df[\"file_name\"] == image_name]\n",
        "\n",
        "        bboxes = [box   for box   in image_annos[\"box\"]]\n",
        "        labels = [label for label in image_annos[\"cat\"]]\n",
        "\n",
        "        container.append(SlideContainer(image_folder/image_name, y=[bboxes, labels], level=res_level,width=patch_size, height=patch_size, sample_func=sample_function))\n",
        "    return container\n",
        "\n",
        "train_scanner = \"Hamamatsu XR\" #@param [\"Hamamatsu XR\", \"Hamamatsu S360\", \"Aperio CS\"]  {allow-input: true}\n",
        "val_scanner = \"Hamamatsu S360\" #@param [\"Hamamatsu XR\", \"Hamamatsu S360\", \"Aperio CS\"]  {allow-input: true}\n",
        "\n",
        "patch_size = 256 #@param [256, 512, 1024]\n",
        "res_level = 0\n",
        "\n",
        "train_annos = df[df[\"scanner\"].isin(train_scanner.split(\",\"))]\n",
        "train_container = create_wsi_container(train_annos)\n",
        "\n",
        "val_annos = df[df[\"scanner\"].isin(val_scanner.split(\",\"))]\n",
        "valid_container = create_wsi_container(val_annos)\n",
        "\n",
        "f\"Created: {len(train_container)} training WSI container and {len(valid_container)} validation WSI container\"\n",
        "\n",
        "\n",
        "train_samples_per_scanner = 50 #@param {type:\"integer\"}\n",
        "val_samples_per_scanner = 10 #@param {type:\"integer\"}\n",
        "\n",
        "train_images = list(np.random.choice(train_container, train_samples_per_scanner))\n",
        "valid_images = list(np.random.choice(valid_container, val_samples_per_scanner))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:33<00:00,  1.49it/s]\n",
            "100%|██████████| 50/50 [00:35<00:00,  1.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKCUHmi72SRG",
        "outputId": "e7bb6ae5-8d99-42bd-9071-e8db41ee831b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 12 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Lets add some basic data [augmentation](https://docs.fast.ai/vision.augment.html)\n",
        "do_flip = True #@param {type:\"boolean\"}\n",
        "flip_vert = True #@param {type:\"boolean\"}\n",
        "max_rotate = 90 #@param {type:\"number\"}\n",
        "max_zoom = 1.1 #@param {type:\"number\"}\n",
        "max_lighting = 0.2 #@param {type:\"number\"}\n",
        "max_warp = 0.2 #@param {type:\"number\"}\n",
        "p_affine = 0.75 #@param {type:\"number\"}\n",
        "p_lighting = 0.75 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "tfms = get_transforms(do_flip=do_flip,\n",
        "                      flip_vert=flip_vert,\n",
        "                      max_rotate=max_rotate,\n",
        "                      max_zoom=max_zoom,\n",
        "                      max_lighting=max_lighting,\n",
        "                      max_warp=max_warp,\n",
        "                      p_affine=p_affine,\n",
        "                      p_lighting=p_lighting)\n",
        "\n",
        "train, valid = ObjectItemListSlide(train_images), ObjectItemListSlide(valid_images)\n",
        "item_list = ItemLists(\".\", train, valid)\n",
        "lls = item_list.label_from_func(lambda x: x.y, label_cls=SlideObjectCategoryList)\n",
        "lls = lls.transform(tfms, tfm_y=True, size=patch_size)\n",
        "data = lls.databunch(bs=batch_size, collate_fn=bb_pad_collate,num_workers=0).normalize()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QYl4QT73EFm"
      },
      "source": [
        "def partial_vgg(input_tensor=None):\n",
        "\n",
        "\n",
        "    input_shape = (None, None, 3)\n",
        "\n",
        "    img_input = Input(shape=input_shape)\n",
        "    \n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "    print(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "    \n",
        "    # We are not using fully connected layers (3 fc layers) as we need feature maps as output from this network.\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yak7U4e73GUU"
      },
      "source": [
        "#RPN layer\n",
        "\n",
        "def rpn_layer(base_layers, num_anchors):\n",
        " \n",
        "    #cnn_used for creating feature maps: vgg, num_anchors: 9\n",
        "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(base_layers)\n",
        "    \n",
        "    #classification layer: num_anchors (9) channels for 0, 1 sigmoid activation output\n",
        "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid')(x)\n",
        "    \n",
        "    #regression layer: num_anchors*4 (36) channels for computing the regression of bboxes\n",
        "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear')(x)\n",
        "\n",
        "    return [x_class, x_regr, base_layers] #classification of object(0 or 1),compute bounding boxes, base layers vgg"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}