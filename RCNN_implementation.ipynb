{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN implementation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkfXL3RTAH0BllcwW9ZBGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ixiondbz/CSE499/blob/main/RCNN_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBK6o0bPmh6L",
        "outputId": "17cbb3c9-8b5a-4ab8-bb76-6fef3291ed7f"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install -U plotly\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import pickle\n",
        "import math\n",
        "import cv2\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop\n",
        "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
        "from keras.utils.layer_utils import get_source_inputs\n",
        "#from keras.engine.topology import get_source_inputs\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.utils import generic_utils\n",
        "#tf.keras.layers.InputSpec\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras import initializers, regularizers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDNPG5s0mnM5",
        "outputId": "f5131171-3ea0-4199-e3ce-c0063a34a7df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "folder = \"MyDrive/MIDOG_Challenge\" #@param {type:\"string\"}\n",
        "midog_folder = Path(\"/drive\") / Path(folder)\n",
        "\n",
        "print(list(midog_folder.glob(\"*.*\")))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n",
            "[PosixPath('/drive/MyDrive/MIDOG_Challenge/MIDOG.sqlite'), PosixPath('/drive/MyDrive/MIDOG_Challenge/MIDOG.json')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gytRH8aXmozQ"
      },
      "source": [
        "!apt-get install python3-openslide\n",
        "from openslide import open_slide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC_Bhdaxm32K"
      },
      "source": [
        "image_folder = midog_folder / \"images\"\n",
        "\n",
        "hamamatsu_rx_ids = list(range(0, 51))\n",
        "hamamatsu_360_ids = list(range(51, 101))\n",
        "aperio_ids = list(range(101, 151))\n",
        "leica_ids = list(range(151, 201))\n",
        "\n",
        "\n",
        "annotation_file = midog_folder / \"MIDOG.json\"\n",
        "rows = []\n",
        "with open(annotation_file) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    categories = {1: 'mitotic figure', 2: 'hard negative'}\n",
        "\n",
        "    for row in data[\"images\"]:\n",
        "        file_name = row[\"file_name\"]\n",
        "        image_id = row[\"id\"]\n",
        "        width = row[\"width\"]\n",
        "        height = row[\"height\"]\n",
        "\n",
        "        scanner  = \"Hamamatsu XR\"\n",
        "        if image_id in hamamatsu_360_ids:\n",
        "            scanner  = \"Hamamatsu S360\"\n",
        "        if image_id in aperio_ids:\n",
        "            scanner  = \"Aperio CS\"\n",
        "        if image_id in leica_ids:\n",
        "            scanner  = \"Leica GT450\"\n",
        "         \n",
        "        for annotation in [anno for anno in data['annotations'] if anno[\"image_id\"] == image_id]:\n",
        "            box = annotation[\"bbox\"]\n",
        "            cat = categories[annotation[\"category_id\"]]\n",
        "\n",
        "            rows.append([file_name, image_id, width, height, box, cat, scanner])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"file_name\", \"image_id\", \"width\", \"height\", \"box\", \"cat\", \"scanner\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nATeQaHhAZTs",
        "outputId": "7226df2e-bcb4-42b5-fa21-8b44f3638ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "file_name    object\n",
              "image_id      int64\n",
              "width         int64\n",
              "height        int64\n",
              "box          object\n",
              "cat          object\n",
              "scanner      object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6fNXOz5A9tj",
        "outputId": "0f786736-e0bb-41c1-d8d3-54e5a0742de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['box']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               [4336, 346, 4386, 396]\n",
              "1                 [756, 872, 806, 922]\n",
              "2               [270, 4044, 320, 4094]\n",
              "3       [6672.5, 706.5, 6722.5, 756.5]\n",
              "4               [1872, 319, 1922, 369]\n",
              "                     ...              \n",
              "4430            [569, 4276, 619, 4326]\n",
              "4431          [3730, 4538, 3780, 4588]\n",
              "4432          [4318, 4138, 4368, 4188]\n",
              "4433          [5643, 1318, 5693, 1368]\n",
              "4434      [2681.5, 2846, 2731.5, 2896]\n",
              "Name: box, Length: 4435, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCFhmdrmsBHp"
      },
      "source": [
        "!pip install -U object-detection-fastai\n",
        "\n",
        "from object_detection_fastai.helper.wsi_loader import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmtC7JGGqA-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2309997f-8d6e-4104-efa8-306660fcdfae"
      },
      "source": [
        "def sample_function(y, classes, size, level_dimensions, level):\n",
        "    width, height = level_dimensions[level]\n",
        "    if len(y[0]) == 0:\n",
        "        return randint(0, width - size[0]), randint(0, height -size[1])\n",
        "    else:\n",
        "        #if randint(0, 5) < 2:\n",
        "        if True:\n",
        "            class_id = np.random.choice(classes, 1)[0] # select a random class\n",
        "            ids = np.array(y[1]) == class_id # filter the annotations according to the selected class\n",
        "            xmin, ymin, _, _ = np.array(y[0])[ids][randint(0, np.count_nonzero(ids) - 1)] # randomly select one of the filtered annotatons as seed for the training patch\n",
        "            \n",
        "            # To have the selected annotation not in the center of the patch and an random offset.\n",
        "            xmin += random.randint(-size[0]/2, size[0]/2) \n",
        "            ymin += random.randint(-size[1]/2, size[1]/2)\n",
        "            xmin, ymin = max(0, int(xmin - size[0] / 2)), max(0, int(ymin -size[1] / 2))\n",
        "            xmin, ymin = min(xmin, width - size[0]), min(ymin, height - size[1])\n",
        "            return xmin, ymin\n",
        "        else:\n",
        "            return randint(0, width - size[0]), randint(0, height -size[1])\n",
        "\n",
        "            \n",
        "def create_wsi_container(annotations_df: pd.DataFrame):\n",
        "    container = []\n",
        "\n",
        "    for image_name in tqdm(annotations_df[\"file_name\"].unique()):\n",
        "        image_annos = annotations_df[annotations_df[\"file_name\"] == image_name]\n",
        "\n",
        "        bboxes = [box   for box   in image_annos[\"box\"]]\n",
        "        labels = [label for label in image_annos[\"cat\"]]\n",
        "\n",
        "        container.append(SlideContainer(image_folder/image_name, y=[bboxes, labels], level=res_level,width=patch_size, height=patch_size, sample_func=sample_function))\n",
        "    return container\n",
        "\n",
        "train_scanner = \"Hamamatsu XR\" #@param [\"Hamamatsu XR\", \"Hamamatsu S360\", \"Aperio CS\"]  {allow-input: true}\n",
        "val_scanner = \"Hamamatsu S360\" #@param [\"Hamamatsu XR\", \"Hamamatsu S360\", \"Aperio CS\"]  {allow-input: true}\n",
        "\n",
        "patch_size = 256 #@param [256, 512, 1024]\n",
        "res_level = 0\n",
        "\n",
        "train_annos = df[df[\"scanner\"].isin(train_scanner.split(\",\"))]\n",
        "train_container = create_wsi_container(train_annos)\n",
        "\n",
        "val_annos = df[df[\"scanner\"].isin(val_scanner.split(\",\"))]\n",
        "valid_container = create_wsi_container(val_annos)\n",
        "\n",
        "f\"Created: {len(train_container)} training WSI container and {len(valid_container)} validation WSI container\"\n",
        "\n",
        "\n",
        "train_samples_per_scanner = 50 #@param {type:\"integer\"}\n",
        "val_samples_per_scanner = 10 #@param {type:\"integer\"}\n",
        "\n",
        "train_images = list(np.random.choice(train_container, train_samples_per_scanner))\n",
        "valid_images = list(np.random.choice(valid_container, val_samples_per_scanner))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:16<00:00,  1.53s/it]\n",
            "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKCUHmi72SRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd705733-6931-4f03-ee0d-6a996f8eef31"
      },
      "source": [
        "batch_size = 12 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Lets add some basic data [augmentation](https://docs.fast.ai/vision.augment.html)\n",
        "do_flip = True #@param {type:\"boolean\"}\n",
        "flip_vert = True #@param {type:\"boolean\"}\n",
        "max_rotate = 90 #@param {type:\"number\"}\n",
        "max_zoom = 1.1 #@param {type:\"number\"}\n",
        "max_lighting = 0.2 #@param {type:\"number\"}\n",
        "max_warp = 0.2 #@param {type:\"number\"}\n",
        "p_affine = 0.75 #@param {type:\"number\"}\n",
        "p_lighting = 0.75 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "tfms = get_transforms(do_flip=do_flip,\n",
        "                      flip_vert=flip_vert,\n",
        "                      max_rotate=max_rotate,\n",
        "                      max_zoom=max_zoom,\n",
        "                      max_lighting=max_lighting,\n",
        "                      max_warp=max_warp,\n",
        "                      p_affine=p_affine,\n",
        "                      p_lighting=p_lighting)\n",
        "\n",
        "train, valid = ObjectItemListSlide(train_images), ObjectItemListSlide(valid_images)\n",
        "item_list = ItemLists(\".\", train, valid)\n",
        "lls = item_list.label_from_func(lambda x: x.y, label_cls=SlideObjectCategoryList)\n",
        "lls = lls.transform(tfms, tfm_y=True, size=patch_size)\n",
        "data = lls.databunch(bs=batch_size, collate_fn=bb_pad_collate,num_workers=0).normalize()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/fastai/vision/transform.py:247: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\n",
            "torch.linalg.solve has its arguments reversed and does not return the LU factorization.\n",
            "To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\n",
            "X = torch.solve(B, A).solution\n",
            "should be replaced with\n",
            "X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.)\n",
            "  return _solve_func(B,A)[0][:,0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wppjs4_oG3ma"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}