{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Ixiondbz/CSE499/blob/main/Pytorch%20FRCNN%20Implementation.ipynb",
      "authorship_tag": "ABX9TyPFXIpOnwEjCRasnhHg/Vk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ixiondbz/CSE499/blob/main/Pytorch%20FRCNN%20Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lh_LvDbqSq-",
        "outputId": "76d706f6-c0d9-4841-b095-aed9bd1a9a29"
      },
      "source": [
        "# Install dependencies and \n",
        "!pip install albumentations==0.4.6\n",
        "!pip install pycocotools --quiet\n",
        "\n",
        "# Clone TorchVision repo and copy helper files\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "%cd vision\n",
        "!git checkout v0.3.0\n",
        "%cd ..\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.6)\n",
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "/content/vision\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juA5GVYEqT0i"
      },
      "source": [
        "# basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# helper libraries\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8KtGuHqqg1p"
      },
      "source": [
        "# defining the files directory and testing directory\n",
        "files_dir = '/content/drive/MyDrive/CSE499 Project/train_images'\n",
        "test_dir = '/content/drive/MyDrive/CSE499 Project/test_images'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOITb278lckf"
      },
      "source": [
        "#string = \"150.tiff\"\n",
        "#string[-5:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIR6a1vO1dKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17776827-d1a3-4104-e965-2d5c3be3fb18"
      },
      "source": [
        "# we create a Dataset class which has a __getitem__ function and a __len__ function\n",
        "class MitosisImagesDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, files_dir, width, height, transforms=None):\n",
        "    self.transforms = transforms\n",
        "    self.files_dir  = files_dir\n",
        "    self.height     = height\n",
        "    self.width      = width\n",
        "    \n",
        "    # sorting the images for consistency\n",
        "    # To get images, the extension of the filename is checked to be jpg\n",
        "    self.imgs = [image for image in sorted(os.listdir(files_dir)) if image[-5:]=='.tiff']\n",
        "    \n",
        "    # classes: 0 index is reserved for background\n",
        "    self.classes = [_, 'mitosis']\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.imgs[idx]\n",
        "    image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "    # reading the images and converting them to correct size and color    \n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "    # diving by 255\n",
        "    img_res /= 255.0\n",
        "    \n",
        "    # annotation file\n",
        "    annot_filename = str(int(float(img_name[:-4]))) + '.txt'\n",
        "    annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
        "    \n",
        "    boxes = []\n",
        "    labels = []\n",
        "    \n",
        "    # cv2 image gives size as height x width\n",
        "    wt = img.shape[1]\n",
        "    ht = img.shape[0]\n",
        "    \n",
        "    # box coordinates for xml files are extracted and corrected for image size given\n",
        "    with open(annot_file_path) as f:\n",
        "      for line in f:\n",
        "        labels.append(1)\n",
        "        \n",
        "        parsed = [float(x) for x in line.split(' ')]\n",
        "        x_center = parsed[1]\n",
        "        y_center = parsed[2]\n",
        "        box_wt = parsed[3]\n",
        "        box_ht = parsed[4]\n",
        "\n",
        "        xmin = x_center - box_wt/2\n",
        "        xmax = x_center + box_wt/2\n",
        "        ymin = y_center - box_ht/2\n",
        "        ymax = y_center + box_ht/2\n",
        "        \n",
        "        xmin_corr = int(xmin*self.width)\n",
        "        xmax_corr = int(xmax*self.width)\n",
        "        ymin_corr = int(ymin*self.height)\n",
        "        ymax_corr = int(ymax*self.height)\n",
        "        \n",
        "        boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "    \n",
        "    # convert boxes into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    \n",
        "    # getting the areas of the boxes\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "    \n",
        "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    image_id = torch.tensor([idx])\n",
        "    target[\"image_id\"] = image_id\n",
        "\n",
        "    if self.transforms:\n",
        "      sample = self.transforms(image = img_res,\n",
        "                                bboxes = target['boxes'],\n",
        "                                labels = labels)\n",
        "      img_res = sample['image']\n",
        "      target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "        \n",
        "    return img_res, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "\n",
        "# check dataset\n",
        "dataset = MitosisImagesDataset(files_dir, 7215, 5412)\n",
        "print('Length of dataset:', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[0]\n",
        "print('Image shape:', img.shape)\n",
        "print('Label example:', target)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset: 51 \n",
            "\n",
            "Image shape: (5412, 7215, 3)\n",
            "Label example: {'boxes': tensor([[31284240.,  1872552., 31644990.,  2143152.],\n",
            "        [ 5454540.,  4719264.,  5815290.,  4989864.],\n",
            "        [ 1948050., 21886128.,  2308800., 22156728.],\n",
            "        [48142088.,  3823578., 48502836.,  4094178.]]), 'labels': tensor([1, 1, 1, 1]), 'area': tensor([9.7619e+10, 9.7619e+10, 9.7619e+10, 9.7618e+10]), 'iscrowd': tensor([0, 0, 0, 0]), 'image_id': tensor([0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG1lWYJLncbs"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "Let's make some a helper function to view our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf9FtTInfYh"
      },
      "source": [
        "# Function to visualize bounding boxes in the image\n",
        "def plot_img_bbox(img, target):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  plt.figure(figsize=(20,9))\n",
        "  #fig, a = plt.subplots(1,1)\n",
        "  #fig.set_size_inches(5,5)\n",
        "  plt.imshow(img)\n",
        "  for box in (target['boxes']):\n",
        "    x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    plt.gca().add_patch(rect)\n",
        "  \n",
        "  plt.show()\n",
        "    \n",
        "# plotting the image with bboxes. Feel free to change the index\n",
        "img, target = dataset[0]\n",
        "plot_img_bbox(img, target)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_n6HzFLl9G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}