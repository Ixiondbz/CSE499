{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "mount_file_id": "12Spw16sBRUSR0ULZ9yDk7YworVfPEqEZ",
      "authorship_tag": "ABX9TyNAsi8eIv5IHkWt8OLqBS3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ixiondbz/CSE499/blob/main/Pytorch%20FRCNN%20Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKfYq_GdTGqO",
        "outputId": "73abcecc-55b0-46b1-e491-59ab2ca303d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lh_LvDbqSq-",
        "outputId": "f86b703a-5565-4eb6-aae2-eaf7824f41cc"
      },
      "source": [
        "# Install dependencies and \n",
        "!pip install albumentations==0.4.6\n",
        "!pip install pycocotools --quiet\n",
        "\n",
        "# Clone TorchVision repo and copy helper files\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "%cd vision\n",
        "!git checkout v0.3.0\n",
        "%cd ..\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 60221, done.\u001b[K\n",
            "remote: Total 60221 (delta 0), reused 0 (delta 0), pack-reused 60221\u001b[K\n",
            "Receiving objects: 100% (60221/60221), 110.99 MiB | 31.55 MiB/s, done.\n",
            "Resolving deltas: 100% (48765/48765), done.\n",
            "/content/vision\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juA5GVYEqT0i"
      },
      "source": [
        "# basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# helper libraries\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8KtGuHqqg1p"
      },
      "source": [
        "# defining the files directory and testing directory\n",
        "files_dir = '/content/drive/MyDrive/CSE499 Project/train_images'\n",
        "test_dir = '/content/drive/MyDrive/CSE499 Project/test_images'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIR6a1vO1dKM"
      },
      "source": [
        "# we create a Dataset class which has a __getitem__ function and a __len__ function\n",
        "class MitosisImagesDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, files_dir, width, height, transforms=None):\n",
        "    self.transforms = transforms\n",
        "    self.files_dir  = files_dir\n",
        "    self.height     = height\n",
        "    self.width      = width\n",
        "    \n",
        "    # sorting the images for consistency\n",
        "    # To get images, the extension of the filename is checked to be jpg\n",
        "    self.imgs = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.tiff']\n",
        "    \n",
        "    # classes: 0 index is reserved for background\n",
        "    self.classes = [_, 'mitosis']\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.imgs[idx]\n",
        "    image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "    # reading the images and converting them to correct size and color    \n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "    # diving by 255\n",
        "    img_res /= 255.0\n",
        "    \n",
        "    # annotation file\n",
        "    annot_filename = img_name[:-4] + '.txt'\n",
        "    annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
        "    \n",
        "    boxes = []\n",
        "    labels = []\n",
        "    \n",
        "    # cv2 image gives size as height x width\n",
        "    wt = img.shape[1]\n",
        "    ht = img.shape[0]\n",
        "    \n",
        "    # box coordinates for xml files are extracted and corrected for image size given\n",
        "    with open(annot_file_path) as f:\n",
        "      for line in f:\n",
        "        labels.append(1)\n",
        "        \n",
        "        parsed = [float(x) for x in line.split(' ')]\n",
        "        x_center = parsed[1]\n",
        "        y_center = parsed[2]\n",
        "        box_wt = parsed[3]\n",
        "        box_ht = parsed[4]\n",
        "\n",
        "        xmin = x_center - box_wt/2\n",
        "        xmax = x_center + box_wt/2\n",
        "        ymin = y_center - box_ht/2\n",
        "        ymax = y_center + box_ht/2\n",
        "        \n",
        "        xmin_corr = int(xmin*self.width)\n",
        "        xmax_corr = int(xmax*self.width)\n",
        "        ymin_corr = int(ymin*self.height)\n",
        "        ymax_corr = int(ymax*self.height)\n",
        "        \n",
        "        boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "    \n",
        "    # convert boxes into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    \n",
        "    # getting the areas of the boxes\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "    \n",
        "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    image_id = torch.tensor([idx])\n",
        "    target[\"image_id\"] = image_id\n",
        "\n",
        "    if self.transforms:\n",
        "      sample = self.transforms(image = img_res,\n",
        "                                bboxes = target['boxes'],\n",
        "                                labels = labels)\n",
        "      img_res = sample['image']\n",
        "      target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "        \n",
        "    return img_res, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "\n",
        "# check dataset\n",
        "dataset = MitosisImagesDataset(files_dir, 224, 224)\n",
        "print('Length of dataset:', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[78]\n",
        "print('Image shape:', img.shape)\n",
        "print('Label example:', target)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}